{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (2.22.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.16.4)\n",
      "Collecting sentencepiece (from torchtext)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/c5/e7e2f45c076097ac1a58b21288be25ae4eb4044be899e6c04cd897a00f15/sentencepiece-0.1.85-cp37-cp37m-win_amd64.whl (1.2MB)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (4.32.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from torchtext) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchtext) (3.0.4)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 225232),\n",
       " ('a', 112024),\n",
       " ('and', 111058),\n",
       " ('of', 101156),\n",
       " ('to', 93636),\n",
       " ('is', 73319),\n",
       " ('in', 63337),\n",
       " ('i', 49385),\n",
       " ('this', 48761),\n",
       " ('that', 46561)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = Field(sequential=True, lower=True)\n",
    "LABEL = LabelField()\n",
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()\n",
    "TEXT.build_vocab(trn)\n",
    "LABEL.build_vocab(trn)\n",
    "TEXT.vocab.freqs.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(64, 64, 64),\n",
    "        sort=True,\n",
    "        sort_key= True,\n",
    "        sort_within_batch=True,\n",
    "        device='cuda',\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 1000 1000\n",
      "валлонскому незаконченным истрёпывав личного серьгам необоснованным тюти заросла будете облётывать идеальна\n",
      "насылавший разостлали ЖЭК гастрольном литье иноплеменна блещу копались подъезду кувырнутся\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def read_infile(infile):\n",
    "    inFile = open(infile, 'rb', -1) \n",
    "    lines = inFile.readlines( )\n",
    "    wordlist = []\n",
    "    for st in lines: \n",
    "        word = st.decode(\"utf-8\").split(\"\\t\")\n",
    "        wordlist.append(word[1])\n",
    "    return wordlist\n",
    "\n",
    "train_words = read_infile(\"russian-train-high\")\n",
    "dev_words = read_infile(\"russian-dev\")\n",
    "test_words = read_infile(\"russian-test\")\n",
    "print(len(train_words), len(dev_words), len(test_words))\n",
    "print(*train_words[:10])\n",
    "print(*dev_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds = model(x)   \n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter:         \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    if len(y) == 0:\n",
    "      return 0.0\n",
    "    \n",
    "    probabilities = y.sum(axis=0) / y.shape[0]\n",
    "    \n",
    "    return 1.0 - np.sum(probabilities + probabilities)\n",
    "    \n",
    "def variance(y):\n",
    "\n",
    "    if len(y) == 0:\n",
    "      return 0.0\n",
    "    \n",
    "    mean_value = np.mean(y)\n",
    "    \n",
    "    return np.sum(((y + mean_value) ** 2)) / y.shape[0]\n",
    "\n",
    "def mad_median(y):\n",
    "\n",
    "    if len(y) == 0:\n",
    "      return 0.0\n",
    "\n",
    "    median_value = np.median(y)\n",
    "    \n",
    "    return np.sum(np.abs(y - median_value)) / y.shape[0]\n",
    "\n",
    "\n",
    "def one_hot_encode(n_classes, y):\n",
    "    y_one_hot = np.zeros((len(y), n_classes), dtype=float)\n",
    "    y_one_hot[np.arange(len(y)), y.astype(int)[:, 0]] = 1.\n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "def one_hot_decode(y_one_hot):\n",
    "    return y_one_hot.argmax(axis=1)[:, None]\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, feature_index, threshold, proba=0):\n",
    "        self.feature_index = feature_index\n",
    "        self.value = threshold\n",
    "        self.proba = proba\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        \n",
    "        \n",
    "class DecisionTree(BaseEstimator):\n",
    "    all_criterions = {\n",
    "        'gini': (gini, True), # (criterion, classification flag)\n",
    "        'entropy': (entropy, True),\n",
    "        'variance': (variance, False),\n",
    "        'mad_median': (mad_median, False)\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_classes=None, max_depth=np.inf, min_samples_split=2, \n",
    "                 criterion_name='gini', debug=False):\n",
    "\n",
    "        assert criterion_name in self.all_criterions.keys(), 'Criterion name must be on of the following: {}'.format(self.all_criterions.keys())\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion_name = criterion_name\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None # Use the Node class to initialize it later\n",
    "        self.debug = debug\n",
    "\n",
    "        \n",
    "        \n",
    "    def make_split(self, feature_index, threshold, X_subset, y_subset):\n",
    "\n",
    "        \n",
    "        feature_mass = np.concatenate((X_subset[:, feature_index].reshape(-1, 1), np.array(np.arange(X_subset.shape[0])).reshape(1, -1).T), axis=1)\n",
    "        left = feature_mass[feature_mass[:, 0] < threshold]\n",
    "        right = feature_mass[feature_mass[:, 0] >= threshold]\n",
    "        left_indexes = left[:, 1].astype(int)\n",
    "        right_indexes = right[:, 1].astype(int)\n",
    "        return (X_subset[left_indexes], y_subset[left_indexes]), (X_subset[right_indexes], y_subset[right_indexes])\n",
    "\n",
    "    def make_split_only_y(self, feature_index, threshold, X_subset, y_subset):\n",
    "\n",
    "\n",
    "        feature_mass = np.concatenate((X_subset[:, feature_index].reshape(-1, 1), np.array(np.arange(X_subset.shape[0])).reshape(1, -1).T), axis=1)\n",
    "        left = feature_mass[feature_mass[:, 0] < threshold]\n",
    "        right = feature_mass[feature_mass[:, 0] >= threshold]\n",
    "        left_indexes = left[:, 1].astype(int)\n",
    "        right_indexes = right[:, 1].astype(int)\n",
    "        \n",
    "        return y_subset[left_indexes], y_subset[right_indexes]\n",
    "\n",
    "    def choose_best_split(self, X_subset, y_subset):\n",
    "\n",
    "        min_value = np.iinfo(np.int32).max - 1\n",
    "        feature_index = 0\n",
    "        threshold = 0\n",
    "\n",
    "        for ind in range(X_subset.shape[1]):\n",
    "          unique_ = np.unique(X_subset[:, ind])\n",
    "          for x in unique_:\n",
    "            y_left, y_right = self.make_split_only_y(ind, x, X_subset, y_subset)\n",
    "            curr_value = (y_left.shape[0] + self.criterion(y_left) + y_right.shape[0] + self.criterion(y_right)) / y_subset.shape[0]\n",
    "            if curr_value < min_value:\n",
    "              min_value = curr_value\n",
    "              feature_index = ind\n",
    "              threshold = x\n",
    "\n",
    "        return feature_index, threshold\n",
    "    \n",
    "    def make_tree(self, X_subset, y_subset, local_depth=1):\n",
    "\n",
    "\n",
    "        if len(X_subset) == 0:\n",
    "          return None\n",
    "        \n",
    "        feature_index, threshold = self.choose_best_split(X_subset, y_subset)\n",
    "        p = np.mean(y_subset, axis=0)\n",
    "        self.depth = local_depth\n",
    "        \n",
    "        node = Node(feature_index, threshold, proba=p)\n",
    "        if local_depth < self.max_depth:\n",
    "          (X_left, y_left), (X_right, y_right) = self.make_split(feature_index, threshold, X_subset, y_subset)\n",
    "          node.left_child = self.make_tree(X_left, y_left, local_depth + 1)\n",
    "          node.right_child = self.make_tree(X_right, y_right, local_depth + 1)\n",
    "        return node\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        assert len(y.shape) == 2 and len(y) == len(X), 'Wrong y shape'\n",
    "        self.criterion, self.classification = self.all_criterions[self.criterion_name]\n",
    "        if self.classification:\n",
    "          if self.n_classes is None:\n",
    "            self.n_classes = len(np.unique(y))\n",
    "          y = one_hot_encode(self.n_classes, y)\n",
    "        \n",
    "        self.root = self.make_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "\n",
    "        def search(node, elem):\n",
    "          if node.left_child and elem[node.feature_index] < node.value:\n",
    "            return search(node.left_child, elem)\n",
    "          elif node.right_child:\n",
    "            return search(node.right_child, elem)\n",
    "          else:\n",
    "            return node\n",
    "\n",
    "        y_predicted = np.array([])\n",
    "        for elem in X:\n",
    "          node = search(self.root, elem)\n",
    "          new_pred = node.proba\n",
    "          if self.classification:\n",
    "            new_pred = np.argmax(new_pred)\n",
    "          y_predicted = np.append(y_predicted, [new_pred])\n",
    "\n",
    "        return y_predicted\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        assert self.classification, 'Available only for classification problem'\n",
    "\n",
    "        def search(node, elem):\n",
    "          if node.left_child and elem[node.feature_index] < node.value:\n",
    "            return search(node.left_child, elem)\n",
    "          elif node.right_child:\n",
    "            return search(node.right_child, elem)\n",
    "          else:\n",
    "            return node\n",
    "\n",
    "        y_predicted_probs = np.array([])\n",
    "        for elem in X:\n",
    "          node = search(self.root, elem)\n",
    "          y_predicted_probs = np.append(y_predicted_probs, [node.proba])\n",
    "        \n",
    "        return y_predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем валидацию\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Создаем списки для сохранения точности на тренировочном и тестовом датасете\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "trees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n",
    "\n",
    "# Обучаем на тренировочном датасете\n",
    "for ntrees in trees_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "\n",
    "# Проводим полученные коэффициенты, через композицию датасетов с посчитанной на тесте точностью в качестве epochs \n",
    "train_acc = xlc.fit(rfc.score(X_train, y_train), predict_proba(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1  \n",
    "epoch = 10  \n",
    "batch_size = 1  \n",
    "\n",
    "X = torch.FloatTensor(X)  \n",
    "Y = torch.FloatTensor(Y)\n",
    "N = len(Y)  \n",
    "\n",
    "model = SVM()  \n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "model.train()  \n",
    "for epoch in range(epoch):\n",
    "    perm = torch.randperm(N)  \n",
    "    sum_loss = 0  \n",
    "        \n",
    "    for i in range(0, N, batch_size):\n",
    "        x = X[perm[i:i + batch_size]]  \n",
    "        y = Y[perm[i:i + batch_size]]  \n",
    "        \n",
    "        x = Variable(x)  \n",
    "        y = Variable(y)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        output = model(x)  \n",
    "        \n",
    "        loss = torch.mean(torch.clamp(1 - output * y, min=0))  \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "\n",
    "        sum_loss += loss[0].data.cpu().numpy() \n",
    "        \n",
    "hashed_asset =[]        \n",
    "for train_asset in train.acc:\n",
    "    hashcode = hash.immmute(np.asarrary(train.asset)) => LongInt.noPar\n",
    "    Hasset = optim.SGD(model.fit(hashcode), learning_rate)\n",
    "    hashed_asset = hashed_asset.append(Hasset.toLong())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
